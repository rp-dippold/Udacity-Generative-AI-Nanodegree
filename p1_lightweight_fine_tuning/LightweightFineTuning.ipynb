{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: `LoRA`\n",
    "* Model: `GPT-2` \n",
    "* Evaluation approach: `Accuracy on test set` \n",
    "* Fine-tuning dataset: `yelp_review_full`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9835c34f-a4fd-46db-8458-c2a122650056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages for this notebook\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from evaluate import evaluator\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from peft import get_peft_model, AutoPeftModelForSequenceClassification\n",
    "from peft import LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e88b03-61ed-4deb-8eaa-1d72de8ea516",
   "metadata": {},
   "source": [
    "## Some helper functions used in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5701dc04-ae8e-470f-b405-484e23e73956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"Determine and print out the number of trainable trainable and all parameters.\"\"\"\n",
    "    # Code was taken form huggingface (https://huggingface.co/).\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "40393692-993e-4645-9937-56ffaefa6e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Returns the accuracy of a model by providing predictions and ground truth labels.\"\"\"\n",
    "    # Code was taken form the Udacity example notebooks.\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {'accuracy': (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4824d61f-239e-4191-8b7b-7a7c699b1923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tokenizer, model, text):\n",
    "    \"\"\"Returns the logits, the predicted class id and the corresponding label.\"\"\"\n",
    "    inputs = tokenizer(text, padding='max_length', truncation=True, return_tensors='pt').to('cuda:0')\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        predicted_class_id = logits.argmax().item()\n",
    "\n",
    "    return {'logits': logits, 'class_id': predicted_class_id, 'label': model.config.id2label[predicted_class_id]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce24345-7caa-41ab-8e31-28b5cc41fd7f",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 65000\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['label', 'text'],\n",
       "     num_rows: 5000\n",
       " })}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the train and test splits of the yelp_review_full dataset\n",
    "\n",
    "splits = ['train', 'test']\n",
    "sizes_percent = {'train': 0.1, 'test': 0.1} # only a fraction of the data is taken to reduce computational resources needed\n",
    "\n",
    "ds = {split: ds for split, ds in zip(\n",
    "      splits, load_dataset('yelp_review_full', split=splits, trust_remote_code=True))}\n",
    "\n",
    "# Thin out the dataset to make it run faster\n",
    "for split in splits:\n",
    "    ds[split] = ds[split].shuffle(seed=42).select(range(int(ds[split].shape[0]*sizes_percent[split])))\n",
    "\n",
    "# Show the dataset\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "0af26656-4f2e-4ad3-97a5-0f2887196589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: train \t Label: 4 \t Amount: 12900\n",
      "Split: train \t Label: 2 \t Amount: 12875\n",
      "Split: train \t Label: 0 \t Amount: 13109\n",
      "Split: train \t Label: 3 \t Amount: 13108\n",
      "Split: train \t Label: 1 \t Amount: 13008\n",
      "Split: test \t Label: 2 \t Amount: 974\n",
      "Split: test \t Label: 4 \t Amount: 953\n",
      "Split: test \t Label: 1 \t Amount: 986\n",
      "Split: test \t Label: 3 \t Amount: 1055\n",
      "Split: test \t Label: 0 \t Amount: 1032\n"
     ]
    }
   ],
   "source": [
    "# Check the number of samples for each split and class; make sure that they are equally distributed\n",
    "from collections import Counter\n",
    "for split in splits:\n",
    "    lbls = dict(Counter(ds[split]['label']))\n",
    "    for lbl in lbls:\n",
    "        print(f'Split: {split} \\t Label: {lbl} \\t Amount: {lbls[lbl]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd940943-35cc-4cc0-bf13-a44f812b979c",
   "metadata": {},
   "source": [
    "### Creating a tokenizer and tokenized datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c42b8a4f-cdbc-47da-8db5-cb87f5de37c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b177df1f3c314f35b35944f2613b64ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/65000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26ef69b1ffe44f30b5d8f6b23f521515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['labels', 'input_ids'],\n",
       "     num_rows: 65000\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['labels', 'input_ids'],\n",
       "     num_rows: 5000\n",
       " })}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tokenizer for GPT2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "def preprocess_function(samples):\n",
    "    \"\"\"Preprocess the yelp dataset by returning tokenized samples.\"\"\"\n",
    "    return tokenizer(samples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# Create tokenized datasets.\n",
    "tokenized_ds = {}\n",
    "for split in splits:\n",
    "    tokenized_ds[split] = ds[split].map(preprocess_function, batched=True)\n",
    "    # rename and remove columns\n",
    "    tokenized_ds[split] = tokenized_ds[split].rename_column('label', 'labels')\n",
    "    tokenized_ds[split] = tokenized_ds[split].remove_columns(['attention_mask', 'text'])\n",
    "\n",
    "# Show the dataset\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922a8618-c41d-4784-9c97-b3f940cce0c9",
   "metadata": {},
   "source": [
    "### Loading the base model and customizing it for the classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20f425d3-082e-4715-8910-cc4d99317cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2ForSequenceClassification(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (score): Linear(in_features=768, out_features=5, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load a gpt-2 foundation model for sequence classification\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\n",
    "    'gpt2',\n",
    "    num_labels=5,\n",
    "    id2label={0: '1 star', 1: '2 stars', 2: '3 stars', 3: '4 stars', 4: '5 stars'},\n",
    "    label2id={'1 star': 0, '2 stars': 1, '3 stars': 2, '4 stars': 3, '5 stars': 4},\n",
    ")\n",
    "\n",
    "# Inform the model about the pad_token_id specified in the tokenizer!\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Freeze all the parameters of the base model\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Print the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03f03472-6ec4-4655-9688-191a81f64363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3840 || all params: 124443648 || trainable%: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Show the amount of trainable parameters.\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfe196b-0319-4ddf-80df-e86f57fd3109",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2b34edb-bb59-44b7-a89b-805a89ef1a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HuggingFace Trainer to handle the training and evaluation loop for PyTorch.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir='./data/sentiment_analysis_base',\n",
    "        learning_rate=2e-3,\n",
    "        per_device_train_batch_size=40,\n",
    "        per_device_eval_batch_size=40,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        load_best_model_at_end=True,\n",
    "        label_names=[\"labels\"]\n",
    "    ),\n",
    "    train_dataset=tokenized_ds['train'],\n",
    "    eval_dataset=tokenized_ds['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07ad1e36-c156-4be2-aa37-b8b6388c2042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8125' max='8125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8125/8125 4:06:04, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.132200</td>\n",
       "      <td>1.165133</td>\n",
       "      <td>0.495600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.105900</td>\n",
       "      <td>1.075997</td>\n",
       "      <td>0.528000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.079000</td>\n",
       "      <td>1.069036</td>\n",
       "      <td>0.527200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.061900</td>\n",
       "      <td>1.044171</td>\n",
       "      <td>0.539400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.033900</td>\n",
       "      <td>1.032026</td>\n",
       "      <td>0.550800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8125, training_loss=1.1012529146634615, metrics={'train_runtime': 14766.1504, 'train_samples_per_second': 22.01, 'train_steps_per_second': 0.55, 'total_flos': 1.69847488512e+17, 'train_loss': 1.1012529146634615, 'epoch': 5.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7fc77684-8f84-4179-be06-533b513438d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "#model.save_pretrained('./models/gpt-2-pretrained')\n",
    "\n",
    "# Save tokenizer\n",
    "#tokenizer.save_pretrained('./models/gpt-2-tokenizer')\n",
    "\n",
    "# Optionally, load the trained model later\n",
    "#model_reloaded = GPT2ForSequenceClassification.from_pretrained('./models/gpt-2-pretrained').to('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6ba4a6-99e4-4234-b7d7-877403b372df",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc7cc2e1-c188-4875-8b3b-5fc3486b39c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 03:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.032025933265686,\n",
       " 'eval_accuracy': 0.5508,\n",
       " 'eval_runtime': 192.5453,\n",
       " 'eval_samples_per_second': 25.968,\n",
       " 'eval_steps_per_second': 0.649,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the performance of the model on the test set\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0ddbaf4-cb07-4b80-9ca9-7660190c5236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ 3.6169,  4.5685,  6.8587,  9.6815, 11.6487]], device='cuda:0')\n",
      "Class-Id: 4\n",
      "Label: 5 stars\n"
     ]
    }
   ],
   "source": [
    "# Test an example text from yelp\n",
    "\n",
    "example = '''Top notch doctor in a top notch practice. Can't say I am surprised when \\\n",
    "I was referred to him by another doctor who I think is wonderful and because he went \\\n",
    "to one of the best medical schools in the country. \\nIt is really easy to get an appointment. \\\n",
    "There is minimal wait to be seen and his bedside manner is great.'''\n",
    "\n",
    "# yelp label: 5 stars\n",
    "\n",
    "# Get model prediction\n",
    "prediction = predict(tokenizer, model, example)\n",
    "\n",
    "print(f'Logits: {prediction[\"logits\"]}')\n",
    "print(f'Class-Id: {prediction[\"class_id\"]}')\n",
    "print(f'Label: {prediction[\"label\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e4f41-b4d8-4bfc-8127-fbfd864e5005",
   "metadata": {},
   "source": [
    "### Create a PEFT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "bbc01984-640e-4470-99ed-d8d295cff431",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load a gpt-2 foundation model for sequence classification\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\n",
    "    'gpt2',\n",
    "    num_labels=5,\n",
    "    id2label={0: '1 star', 1: '2 stars', 2: '3 stars', 3: '4 stars', 4: '5 stars'},\n",
    "    label2id={'1 star': 0, '2 stars': 1, '3 stars': 2, '4 stars': 3, '5 stars': 4},\n",
    ")\n",
    "\n",
    "# Inform the model about the pad_token_id specified in the tokenizer!\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e49f64f-0e82-4668-81eb-30325b701ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Lora configuration\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    use_rslora=True,\n",
    "    lora_dropout=0.05,\n",
    "    modules_to_save=[\"score\"],\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 593,664 || all params: 125,037,312 || trainable%: 0.4747894772402017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pdippold/miniconda3/envs/udacity_genai/lib/python3.9/site-packages/peft/tuners/lora/layer.py:711: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Get Lora model for gpt-2 using the Lora configuration\n",
    "lora_model = get_peft_model(model, config)\n",
    "\n",
    "# Show the amount of trainable parameters.\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "edc38fb8-06aa-421c-8ffe-cf5d8124f2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForSequenceClassification(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPT2ForSequenceClassification(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(50257, 768)\n",
      "        (wpe): Embedding(1024, 768)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-11): 12 x GPT2Block(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): lora.Linear(\n",
      "                (base_layer): Conv1D()\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (c_proj): Conv1D()\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): Conv1D()\n",
      "              (c_proj): Conv1D()\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (score): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=5, bias=False)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=5, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Print the Lora model\n",
    "print(lora_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd51755-d67f-4b9f-ae66-6d86723b120e",
   "metadata": {},
   "source": [
    "### Fine-tune the Lora Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f77d45a2-740c-4bea-b008-d81606bb5b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HuggingFace Trainer to handle the training and evaluation loop for PyTorch.\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir='./data/sentiment_analysis_lora',\n",
    "        learning_rate=5e-4,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.05,\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        load_best_model_at_end=True,\n",
    "        label_names=[\"labels\"]\n",
    "    ),\n",
    "    train_dataset=tokenized_ds['train'],\n",
    "    eval_dataset=tokenized_ds['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "119d7fba-ece5-4e1f-ba24-f51dc3b73bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='81250' max='81250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [81250/81250 10:30:47, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.897100</td>\n",
       "      <td>0.888117</td>\n",
       "      <td>0.612200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.865300</td>\n",
       "      <td>0.903920</td>\n",
       "      <td>0.622600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.827800</td>\n",
       "      <td>0.854013</td>\n",
       "      <td>0.651000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.756800</td>\n",
       "      <td>0.854632</td>\n",
       "      <td>0.658000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.740200</td>\n",
       "      <td>0.866431</td>\n",
       "      <td>0.660600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=81250, training_loss=0.8381380267803485, metrics={'train_runtime': 37848.7196, 'train_samples_per_second': 8.587, 'train_steps_per_second': 2.147, 'total_flos': 1.710329167872e+17, 'train_loss': 0.8381380267803485, 'epoch': 5.0})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcf9ed1-2a82-41c8-9967-ec58cb275a23",
   "metadata": {},
   "source": [
    "### Save the model and the fine-tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4c591a4d-00e9-40a8-9ec2-f2cc8d219afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fine-tuned parameters\n",
    "#lora_model.save_pretrained(\"./models/gpt-2-lora-ft-parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a76d51ff-0663-48c1-9fd9-163ca6944b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the complete model\n",
    "#merged_model = lora_model.merge_and_unload()\n",
    "#merged_model.save_pretrained('./models/gpt-2-lora-full')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e0b40-1d96-42bf-b0ab-dd4640f5281c",
   "metadata": {},
   "source": [
    "### Calculate accuracy of trained base model (without PEFT) for test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9395aa6c-0986-4962-a76e-2de0eee7fffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "model_reloaded = GPT2ForSequenceClassification.from_pretrained('./models/gpt-2-pretrained').to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c0af4a58-8310-4efd-a261-17abfa8a422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer if not already loaded above\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('models/gpt-2-tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1c835183-bf71-4ab2-b43a-0917b4f794d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5508, 'total_time_in_seconds': 74.39159498299705, 'samples_per_second': 67.21189404720789, 'latency_in_seconds': 0.01487831899659941}\n"
     ]
    }
   ],
   "source": [
    "label_mapping = {'1 star': 0, '2 stars': 1, '3 stars': 2, '4 stars': 3, '5 stars': 4}\n",
    "task_evaluator = evaluator(\"text-classification\")\n",
    "results = task_evaluator.compute(model_or_pipeline=model_reloaded,\n",
    "                                 tokenizer=tokenizer,\n",
    "                                 data=ds['test'],\n",
    "                                 input_column='text',\n",
    "                                 label_column='label',\n",
    "                                 metric='accuracy',\n",
    "                                 label_mapping=label_mapping,)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbaa910-2606-4a4d-b18a-b1138c01ca86",
   "metadata": {},
   "source": [
    "### Load gpt-2 model and add the saved model weights\n",
    "\n",
    "Alternatively you could load the complete Lora model as follows:\n",
    "\n",
    "`lora_reloaded = GPT2ForSequenceClassification.from_pretrained('models/gpt-2-lora-full').to('cuda:0')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model with saved PEFT model weights\n",
    "lora_reloaded = AutoPeftModelForSequenceClassification.from_pretrained(\n",
    "    'models/gpt-2-lora-ft-parameters',\n",
    "    num_labels=5,\n",
    "    id2label={0: '1 star', 1: '2 stars', 2: '3 stars', 3: '4 stars', 4: '5 stars'},\n",
    "    label2id={'1 star': 0, '2 stars': 1, '3 stars': 2, '4 stars': 3, '5 stars': 4},\n",
    ").to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2470ace3-58e5-4d6b-8199-b031b37d985b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6354}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Evaluating the model\n",
    "\n",
    "# Get predictions for each test sample\n",
    "pred_logits = []\n",
    "for sample in ds['test']:\n",
    "    logits = predict(tokenizer, lora_reloaded, sample['text'])['logits']\n",
    "    pred_logits.append(logits.detach().cpu().squeeze().numpy())\n",
    "\n",
    "# Compute accuracy\n",
    "compute_metrics((pred_logits, np.array(ds['test']['label'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd46c16-5b9d-4914-9962-2664296a6dd0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Result\n",
    "\n",
    "After reloading and re-splitting the data into `train`and `test`sets, the preformance of the base model (only fine-tuned to the classification task) with respect to the accuracy metric on the test set is 55.08 % and is thus much worse than the PEFT model's accuracy of 63.54 %.\n",
    "\n",
    "Interestingly, reloading the complete full model as described below, the accuracy is a little bit higher: 65.1 %.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52cdf4f-25d5-48a3-968e-016f0f8e8b8a",
   "metadata": {},
   "source": [
    "Evaluation of the fine tuned model reloaded with `lora_reloaded = GPT2ForSequenceClassification.from_pretrained('models/gpt-2-lora-full').to('cuda:0')` can be done as follows (not supported for PEFT-models):\n",
    "\n",
    "```\n",
    "import evaluate\n",
    "from evaluate import evaluator\n",
    "\n",
    "label_mapping = {'1 star': 0, '2 stars': 1, '3 stars': 2, '4 stars': 3, '5 stars': 4}\n",
    "task_evaluator = evaluator(\"text-classification\")\n",
    "results = task_evaluator.compute(model_or_pipeline=lora_reloaded,\n",
    "                                 tokenizer=tokenizer,\n",
    "                                 data=ds['test'],\n",
    "                                 input_column='text',\n",
    "                                 label_column='label',\n",
    "                                 metric='accuracy',\n",
    "                                 label_mapping=label_mapping,)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "001ce6c1-ae4a-40f2-9231-2c0063b95d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-3.2397, -3.5468, -1.6284,  1.9804,  4.6484]], device='cuda:0')\n",
      "Class-Id: 4\n",
      "Label: 5 stars\n"
     ]
    }
   ],
   "source": [
    "# Test an example text from yelp\n",
    "\n",
    "example = '''Top notch doctor in a top notch practice. Can't say I am surprised when \\\n",
    "I was referred to him by another doctor who I think is wonderful and because he went \\\n",
    "to one of the best medical schools in the country. \\nIt is really easy to get an appointment. \\\n",
    "There is minimal wait to be seen and his bedside manner is great.'''\n",
    "\n",
    "# yelp label: 5 stars\n",
    "\n",
    "# Get model prediction\n",
    "prediction = predict(tokenizer, lora_reloaded, example)\n",
    "\n",
    "print(f'Logits: {prediction[\"logits\"]}')\n",
    "print(f'Class-Id: {prediction[\"class_id\"]}')\n",
    "print(f'Label: {prediction[\"label\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udacity_genai",
   "language": "python",
   "name": "udacity_genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
